{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Importing Libraries**"
      ],
      "metadata": {
        "id": "K0wfjB3YuCpt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBVgMh9AtlQF"
      },
      "outputs": [],
      "source": [
        "# Dictionary operations\n",
        "import itertools\n",
        "\n",
        "# Data preprocessing\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer, one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Model Training\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Bidirectional"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Reading the text document**"
      ],
      "metadata": {
        "id": "X4ExRsBOus22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('SherlockHolmesStory.txt', 'r', encoding='utf-8') as file:\n",
        "  text = file.read()\n",
        "text"
      ],
      "metadata": {
        "id": "FcoMrkbQun0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Tokenizing the word**"
      ],
      "metadata": {
        "id": "SJoncxZTvELd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization is a way of breaking down the sentences in a piece of text into smaller units called tokens.\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "print(\"Total no. of unique words in the whole book :\", total_words)\n",
        "\n",
        "# Printing the first 10 items in the tokenizer.word_index dictionary\n",
        "print(dict(itertools.islice(tokenizer.word_index.items(), 10)), '...')"
      ],
      "metadata": {
        "id": "7iJIL_CWu1mX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Preparing our dataset**"
      ],
      "metadata": {
        "id": "vKwOThAfxxcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# N-Gram:\n",
        "# Text    ==> This is a Big Data AI Book\n",
        "# Unigram ==> This, is, a, Big, Data, AI, Book\n",
        "# Bigram  ==> This is, is a, a Big, Big Data, Data AI, AI Book\n",
        "# Trigram ==> This is a, is a Big, a Big Data, Big Data AI, Data AI Book\n",
        "input_sequences = []\n",
        "token_list_chk = []\n",
        "for line in text.split('\\n'):\n",
        "  token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "  token_list_chk.append(token_list)\n",
        "\n",
        "  # Loop to run the N-grams from bi-gram to the length of the whole sentence\n",
        "  for i in range(1, len(token_list)):\n",
        "    n_gram_sequence = token_list[:i+1]\n",
        "    input_sequences.append(n_gram_sequence)\n",
        "print(\"Sentences after applying separator of \\\\n : \", [token_list_chk[i] for i in range(0, 5)], '|| where [1, 1561, 5, 129, 34] ==> [the adventures of sherlock holmes]')\n",
        "print(\"The sentences after N-gram :\", [input_sequences[i] for i in range(0, 4)])\n",
        "\n",
        "# Extracting the maximum length among the sentences' length\n",
        "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
        "\n",
        "# Padding extra zeroes to the start of sentence and converting the whole thing to a numpy array\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen = max_sequence_len, padding = 'pre'))\n",
        "print(\"Input after padding zeroes :\", input_sequences[0])\n",
        "\n",
        "# ‘X’ contains all tokens in each array except the last one, which represents the 'input' context.\n",
        "# The ‘y’ array is assigned the values of the last column in the input_sequences array that represents the 'target' or the 'predicted' word.\n",
        "X = input_sequences[:, :-1]\n",
        "y = input_sequences[:, -1]"
      ],
      "metadata": {
        "id": "3gPvl7M1vQOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**DO THIS STEP ONLY IF YOU HAVE A HUGE AMOUNT OF RAM**"
      ],
      "metadata": {
        "id": "W8bcC4HfR_P6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encoding our output parameters (Forming categories out of the whole set)\n",
        "''' y = np.array(keras.utils.to_categorical(y, num_classes = total_words))\n",
        "    Note: We skip this step since the categorical encoding of nearly 100,000 output values exhausts all of the RAM memory and system crashes!!'''\n",
        "'''If your system is capable of doing this, then compile the model as:\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "'''"
      ],
      "metadata": {
        "id": "ho-CR4NDDozL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Creating model**"
      ],
      "metadata": {
        "id": "pDqnJQG3-FKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length = max_sequence_len-1))\n",
        "model.add(LSTM(150))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "pI_JitATzNvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Training**"
      ],
      "metadata": {
        "id": "RKj8ZfnO-W7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs = 100, verbose = 1)"
      ],
      "metadata": {
        "id": "PBewr9JD-JVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Output prediction**"
      ],
      "metadata": {
        "id": "1LthhvG2PRaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"I will close the door if\"\n",
        "next_words = 5\n",
        "\n",
        "for _ in range(next_words):\n",
        "  # Convert to token\n",
        "  token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "  # Path sequences\n",
        "  token_list = pad_sequences([token_list], maxlen = max_sequence_len-1, padding = 'pre')\n",
        "  # Model prediction\n",
        "  predicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "  output_word = \"\"\n",
        "  # Get predicted words\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == predicted:\n",
        "      output_word = word\n",
        "      break\n",
        "  seed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "metadata": {
        "id": "4-uP11JUANkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "idQGM6tBPWof"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}